{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Before we move into Regularization, let us see what regression is. \n",
    "Regression helps to analyze the relationship between two or more variables.\n",
    "Regression analyses significant relationships between dependent variable and independent variable and it indicates the strength of impact of multiple independent variables on a dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Linear Regression establishes a relationship between dependent variable (Y) and one or more independent variables (X) using a best fit straight line (also known as regression line).\n",
    "In this technique, the dependent variable is continuous, independent variable(s) can be continuous or discrete, and nature of regression line is linear.\n",
    "It is represented by the equation, Y =  a1X1 + a2X2 + a2X2…..+ anXn + e \n",
    "Here, we have Y as our dependent variable, X’s are the independent variables and all “a” are the coefficients. \n",
    "Coefficients are basically the weights assigned to the features, based on their importance and “e” is error. \n",
    "This equation can be used to predict the value of target variable based on given predictor variable(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Linear Regression with Example ############\n",
    "I have taken a simple “mtcars” data for example, where,\n",
    "The “mpg” [miles per gallon - mileage] is initially predicted with one independent variable “wt” [weight]. \n",
    "Then more independent variables are added to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "mtcars = pd.read_csv(\"data/mtcars.csv\")\n",
    "\n",
    "############### Simple Linear Regression with one independent variable #############################################\n",
    "# DV & IDV Identification\n",
    "# DV: mpg\n",
    "# IDV: wt\n",
    "\n",
    "# Visualize the relationship between DV and IDV\n",
    "plt.scatter(\"wt\",\"mpg\",data = mtcars)\n",
    "plt.xlabel(\"wt\")\n",
    "plt.ylabel(\"mpg\")\n",
    "\n",
    "# Correlation analysis between DV and IDV\n",
    "mtcars[\"mpg\"].corr(mtcars[\"wt\"]) # -0.867 Strong negative correlation\n",
    "\n",
    "# Simple linear Regression Model Building\n",
    "cars_model = smf.ols(formula = \"mpg ~ wt\", data = mtcars).fit()\n",
    "cars_model.summary()\n",
    "\n",
    "# Coefficient Check\n",
    "# mpg = -5.3445*wt + 37.28\n",
    "# For 1 unit increase in weight, mpg will decrease by 5 units\n",
    "# R-Squared: 0.753 # Decent model\n",
    "\n",
    "################ MultiLinear Regression Model Building - Adding more independent variables ##################################\n",
    "# Adding more variables to improve accuracy\n",
    "# DV & IDV Identification\n",
    "mtcars.columns\n",
    "# DV: mpg\n",
    "# IDV: 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear','carb'\n",
    "\n",
    "# Visualize the relationship between DV and IDV\n",
    "f, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8),(ax9,ax10)) = plt.subplots(5,2)\n",
    "ax1.scatter(\"cyl\",\"mpg\",data = mtcars)\n",
    "ax2.scatter(\"disp\",\"mpg\",data = mtcars)\n",
    "ax3.scatter(\"hp\",\"mpg\",data = mtcars)\n",
    "ax4.scatter(\"drat\",\"mpg\",data = mtcars)\n",
    "ax5.scatter(\"wt\",\"mpg\",data = mtcars)\n",
    "ax6.scatter(\"qsec\",\"mpg\",data = mtcars)\n",
    "ax7.scatter(\"vs\",\"mpg\",data = mtcars)\n",
    "ax8.scatter(\"am\",\"mpg\",data = mtcars)\n",
    "ax9.scatter(\"gear\",\"mpg\",data = mtcars)\n",
    "ax9.scatter(\"carb\",\"mpg\",data = mtcars)\n",
    "\n",
    "# Correlation analysis between DV and IDV\n",
    "mtcars_corr_matrix = mtcars.corr()\n",
    "\n",
    "# Multilinear model building\n",
    "cars_model = smf.ols(formula = \"mpg ~ cyl+disp+hp+drat+wt+qsec+vs+am+gear+carb\", data = mtcars).fit()\n",
    "cars_model.summary()\n",
    "\n",
    "# Coefficient Check\n",
    "# mpg = -0.130cyl + 0.024disp -0.0232hp + 0.029drat - 6.917wt + 1.855qsec - 1.898vs + 0.879am + 1.283gear - 0.636carb + 1.454\n",
    "# Adj R-Squared : 807\n",
    "\n",
    "# training-test split\n",
    "all_samples = np.arange(31)\n",
    "np.random.seed(10)\n",
    "0.7*31\n",
    "tr_samples = np.random.choice(all_samples,22,replace=False)\n",
    "mtcars_training_data = mtcars.iloc[tr_samples,:]\n",
    "mtcars_test_data = mtcars.drop(tr_samples)\n",
    "\n",
    "# Linear Regression Model Buildiing on Training data\n",
    "cars_model = smf.ols(formula = \"mpg ~ cyl+disp+hp+drat+wt+qsec+vs+am+gear+carb\", data = mtcars_training_data).fit()\n",
    "cars_model.summary()\n",
    "\n",
    "# Coefficient Check\n",
    "# mpg = -0.111cyl + 0.013disp -0.0215hp + 0.7871drat - 3.715wt + 0.8210qsec + 0.317vs + 2.5202am + 0.655gear - 0.199carb + 12.303\n",
    "# Adj R-Squared : 807\n",
    "\n",
    "pred_mpg = cars_model.predict(mtcars_test_data)\n",
    "\n",
    "mpg_compare = pd.DataFrame({\"Actual_mpg\":mtcars_test_data[\"mpg\"],\"Predicted_mpg\":pred_mpg})\n",
    "\n",
    "# Mean/Median Absolute Percentage Error\n",
    "def MAPE(actual,predicted):    \n",
    "    ape = np.abs(predicted - actual)/actual\n",
    "    ape = ape.replace(np.inf,np.nan)\n",
    "    return([np.mean(ape)*100,np.nanmedian(ape)*100])\n",
    "    \n",
    "MAPE(mtcars_test_data[\"mpg\"],pred_mpg)\n",
    "# 13.3 % error , 86.7% Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "When we have a high dimensional data set, it would be highly inefficient to use all the variables since some of them might be imparting redundant information. \n",
    "We would need to select the right set of variables which give us an accurate model as well as are able to explain the dependent variable well. \n",
    "There are multiple ways to select the right set of variables for the model. \n",
    "First among them would be the business understanding and domain knowledge. \n",
    "We should also take care that the variables we’re selecting should not be correlated among themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Instead of manually selecting the variables, we can automate this process by using forward or backward selection. \n",
    "Forward selection starts with most significant predictor in the model and adds variable for each step. \n",
    "Backward elimination starts with all predictors in the model and removes the least significant variable for each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance in regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bias is error due to overly simplistic assumptions in the learning algorithm [ Having less independenet variables ]. \n",
    "This can lead to the model underfitting the data, making it hard for it to have high predictive accuracy.\n",
    "\n",
    "Variance is error due to too much complexity in the learning algorithm [Adding more independent variables]. \n",
    "This leads to the algorithm being highly sensitive to high degrees of variation in the training data, which can lead the model to overfit the data. we’ll be carrying too much noise from your training data for your model to be very useful for your test data.\n",
    "The bias-variance decomposition essentially decomposes the learning error from any algorithm by adding the bias, the variance and a bit of irreducible error due to noise in the underlying dataset. \n",
    "\n",
    "Essentially, if you make the model more complex and add more variables, you’ll lose bias but gain some variance — in order to get the optimally reduced amount of error, you’ll have to tradeoff bias and variance. \n",
    "You don’t want either high bias or high variance in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Regularization is a mechanisms for avoiding overfitting of the model. \n",
    "Sometimes certain particular variables would dominate the data set. \n",
    "In regularization, what we do is normally we keep the same number of features, but reduce the magnitude of the coefficients.\n",
    "Regularization basically adds the penalty as model complexity increases. \n",
    "Regularization parameter (alpha) penalizes all the parameters except intercept so that model generalizes the data and won’t overfit.\n",
    "\n",
    "There are two types of regularization – L1 [Lasso] & L2 [Ridge]\n",
    "Let us first define L2 [Ridge] regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 [Ridge] Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L2 regularization adds penalty equivalent to square of the magnitude of coefficients\n",
    "• Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "\n",
    "L2 regularization retains all the independent varaibles while adjusting their coefficients to avoid overfitting. \n",
    "\n",
    "The penalty value [alpha] lies between 0 to 1. \n",
    "As the value increases towards 1, the coeficients of the variables tends to reach \"zero\" but does not exactly reach absolute \"Zero\".\n",
    "\n",
    "We will have to iterate through different Alpha values to see better accuracy.\n",
    "If you calculate R-square for each alpha and see which is best.\n",
    "So we have to choose it wisely by iterating it through a range of values and using the one which gives us lowest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### Ridge Regression #######################################\n",
    "X_mtcars_train, X_mtcars_test, y_mtcars_train, y_mtcars_test = \\\n",
    "    train_test_split(mtcars.iloc[:,1:11], \n",
    "                     mtcars.iloc[:,0], \n",
    "                     test_size=0.3, random_state=100)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeReg = Ridge (alpha=0.6, normalize = True)\n",
    "\n",
    "r = ridgeReg.fit(X_mtcars_train,y_mtcars_train)\n",
    "\n",
    "# Coefficient Check\n",
    "ridgeReg.coef_\n",
    "pred_ridge = ridgeReg.predict(X_mtcars_test)\n",
    "\n",
    "# Mean Square Error Calculation\n",
    "mse = np.mean((pred_ridge - y_mtcars_test)**2) # 6.22\n",
    "\n",
    "ridgeReg.score(X_mtcars_test,y_mtcars_test)# 0.828\n",
    "\n",
    "MAPE(y_mtcars_test,pred_ridge)\n",
    "# 8.3 % error , 91.7% Accuracy\n",
    "\n",
    "# Checking the magnitude of coefficients\n",
    "\n",
    "predictors = X_mtcars_train.columns\n",
    "\n",
    "coef = pd.Series(ridgeReg.coef_,predictors).sort_values()\n",
    "\n",
    "coef.plot(kind='bar', title='Modal Coefficients')\n",
    "# magnitude of the coefficients of all the IDVs have been reduced. 'hp' & 'disp' have almost become zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 [Lasso] Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1 regularization adds penalty equivalent to absolute value of the magnitude of coefficients\n",
    "• Minimization objective = LS Obj + α * (sum of absolute value of coefficients)\n",
    "\n",
    "L1 regularization  selects the only some features while reduces the coefficients of others to zero. \n",
    "\n",
    "The penalty value [alpha] lies between 0 to 1. \n",
    "Even for smaller alpha the coeficients of the some variables reaches absolute \"Zero\" and coeficients of other variables moves towards \"Zero\"\n",
    "\n",
    "We will have to iterate through different Alpha values to see better accuracy.\n",
    "If you calculate R-square for each alpha and see which is best.\n",
    "So we have to choose it wisely by iterating it through a range of values and using the one which gives us lowest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### Lasso Regression ########################################\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoReg = Lasso (alpha=0.6, normalize = True)\n",
    "\n",
    "l = lassoReg.fit(X_mtcars_train,y_mtcars_train)\n",
    "\n",
    "# Coefficient Check\n",
    "lassoReg.coef_\n",
    "\n",
    "pred_lasso = lassoReg.predict(X_mtcars_test)\n",
    "\n",
    "# Mean Square Error Calculation\n",
    "mse = np.mean((pred_lasso - y_mtcars_test)**2) # 20.79\n",
    "\n",
    "lassoReg.score(X_mtcars_test,y_mtcars_test) # 0.425\n",
    "\n",
    "MAPE(y_mtcars_test,pred_lasso)\n",
    "# 15.84% error, 84% Accuracy\n",
    "\n",
    "# Checking the magnitude of coefficients\n",
    "\n",
    "predictors = X_mtcars_train.columns\n",
    "\n",
    "coef = pd.Series(lassoReg.coef_,predictors).sort_values()\n",
    "\n",
    "coef.plot(kind='bar', title='Modal Coefficients')\n",
    "# 'wt' and 'cyl' are enough to build the model. Coefficients of rest of the IDVs have become zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Regularization & Dimentionality Reduction [PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Regularisation is the process of penalising complexity in a model so as to prevent overfitting through generalisation.\n",
    "Regularization is used to create constraints on machine learning models to induce (typically) sparseness or robustness.\n",
    "\n",
    "Dimensionality reduction refers to unsupervised learning within a dataset to find a low-dimensional space that adequately captures the data.\n",
    "Dimensionality reduction reduces the number or variables under consideration and is related to feature extraction. \n",
    "It is useful when the data set has similar measurements with different unit e.g Meters, Centimeters etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################# Dimensionality Reduction using PCA #################################\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "newmtcars = mtcars.iloc[:,1:11]\n",
    "mtcars_corr = newmtcars.corr()\n",
    "\n",
    "# Scaling\n",
    "\n",
    "mtcars_scaled= pd.DataFrame(scale(newmtcars))\n",
    "mtcars_scaled.columns = newmtcars.columns\n",
    "\n",
    "# Correlation comparison between scaled and unscaled data:\n",
    "corrmatrix_mtcarsscaled =  mtcars_scaled.corr()\n",
    "corrmatrix_mtcars = newmtcars.corr()\n",
    "# correlation doesn't get affected due to scaling\n",
    "\n",
    "# PCA\n",
    "mtcarspca = PCA().fit(mtcars_scaled)\n",
    "mtcars_projected = pd.DataFrame(mtcarspca.transform(mtcars_scaled))\n",
    "# Dim1, Dim2, Dim3.... Dim13\n",
    "mtcars_projected.columns = [\"Dim\" + str(i) for i in range(1,11)]\n",
    "\n",
    "# Explained Variance Ratio\n",
    "sum(mtcarspca.explained_variance_ratio_)\n",
    "np.cumsum(mtcarspca.explained_variance_ratio_)\n",
    "\n",
    "# 92% accuracy can be obtained with just 4 variables"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
